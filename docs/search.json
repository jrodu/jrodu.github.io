[
  {
    "objectID": "publications/using-regression-spectral-estimation/index.html",
    "href": "publications/using-regression-spectral-estimation/index.html",
    "title": "Using Regression for Spectral Estimation of HMMs",
    "section": "",
    "text": "Hidden Markov Models (HMMs) are widely used to model discrete time series data, but the EM and Gibbs sampling methods used to estimate them are often slow or prone to get stuck in local minima. A more recent class of reduced-dimension spectral methods for estimating HMMs has attractive theoretical properties, but their finite sample size behavior has not been well characterized. We introduce a new spectral model for HMM estimation, a corresponding spectral bilinear regression model, and systematically compare them with a variety of competing simplified models, explaining when and why each method gives superior performance. Using regression to estimate HMMs has a number of advantages, allowing more powerful and flexible modeling."
  },
  {
    "objectID": "publications/two-step-cca/index.html",
    "href": "publications/two-step-cca/index.html",
    "title": "Two step CCA: a new spectral method for estimating vector models of words",
    "section": "",
    "text": "Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank “dictionary” by an eigendecomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification."
  },
  {
    "objectID": "publications/reasoning-using-data/index.html",
    "href": "publications/reasoning-using-data/index.html",
    "title": "Reasoning Using Data: Two Old Ways and One New",
    "section": "",
    "text": "Instead of two cultures, the story of the last couple decades of data science is about the interplay between three different types of reasoning using data. Two of these types of reasoning were well known when Breiman wrote his Two Cultures paper – warranted reasoning (e.g., randomized trials and sampling) and model reasoning (e.g., linear models). Breiman, though he does not appear to have realized it fully, was in fact describing the dynamics arising in a data community that was making progress using the newest, third type of reasoning – outcome reasoning. In this commentary we clarify this dynamic a bit, and suggest some useful language for identifying and differentiating types of problems better suited for outcome reasoning."
  },
  {
    "objectID": "publications/getting-in-shape/index.html",
    "href": "publications/getting-in-shape/index.html",
    "title": "Getting in Shape: Word Embedding SubSpaces",
    "section": "",
    "text": "Many tasks in natural language processing require the alignment of word embeddings. Embedding alignment relies on the geometric properties of the manifold of word vectors. This paper focuses on supervised linear alignment and studies the relationship between the shape of the target embedding. We assess the performance of aligned word vectors on semantic similarity tasks and find that the isotropy of the target embedding is critical to the alignment. Furthermore, aligning with an isotropic noise can deliver satisfactory results. We provide a theoretical framework and guarantees which aid in the understanding of empirical results."
  },
  {
    "objectID": "publications/MDCCA/index.html",
    "href": "publications/MDCCA/index.html",
    "title": "Detecting multivariate cross-correlation between brain regions",
    "section": "",
    "text": "The problem of identifying functional connectivity from multiple time series data recorded in each of two or more brain areas arises in many neuroscientific investigations. For a single stationary time series in each of two brain areas statistical tools such as cross-correlation and Granger causality may be applied. On the other hand, to examine multivariate interactions at a single time point, canonical correlation, which finds the linear combinations of signals that maximize the correlation, may be used. We report here a new method that produces interpretations much like these standard techniques and, in addition, 1) extends the idea of canonical correlation to 3-way arrays (with dimensionality number of signals by number of time points by number of trials), 2) allows for nonstationarity, 3) also allows for nonlinearity, 4) scales well as the number of signals increases, and 5) captures predictive relationships, as is done with Granger causality. We demonstrate the effectiveness of the method through simulation studies and illustrate by analyzing local field potentials recorded from a behaving primate."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Courses at UVA",
    "section": "",
    "text": "Courses at UVA\n* indicates course I created at UVA\n\nCapstone (UVA STAT 4996)\n\nSpring 24\nFall 23\nSpring 23\nSpring 20\n\nApplied Causal Inference (UVA STAT 5350)*\n\nFall 23\nFall 22\n\nNote: this is a course that had not been taught since prior to my joining UVA,\nso it was designed as a new course.\n\n\nAdvanced Sports Analytics (UVA STAT 4800)*\n\nFall 22\nFall 21\nSpring 21\nSpring 20\n\nIntroduction to Sports Analytics (UVA STAT 1800)*\n\nFall 21\nFall 19\n\nStatistical Text Analysis (UVA STAT 4559)*\n\nFall 20\n\nData Visualization and Presentation*\n\nSpring 19\nFall 18\nFall 17\n\nNote: data visualization is now taught along with databases (STAT 3280)\n\n\nStatistical Machine Learning (Graduate level, UVA STAT 5630)\n\nFall 18\n\nStatistical Machine Learning (Undergraduate level, UVA STAT 4630)\n\nSpring 18\n\n\n\n\n\nStatistics PhD students\n\nKyle Peterson (expected 2027)\nTyler Ashoff (expected 2027)\nSydney Campbell (expected 2026)\n\nJoint with Karen Kafadar\n\nIsabella Femia (expected 2026)\nNoah Gade (2024)\nXiaoyuan Ma (2023)\nTianyuan Zhou (2023)\nRuizhong Miao (2022)\n\nJoint with Tianxi Li"
  },
  {
    "objectID": "talks/french_academy_1901.html",
    "href": "talks/french_academy_1901.html",
    "title": "Adventures with Uranium Rays",
    "section": "",
    "text": "Abstract here\n\nSlides\nembed html or pdf slides here\n\n\nVideo"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n    Reid Dale, Jordan Rodu, Maria E. Currie, Mike Baiocchi  Data Gluttony: Epistemic Risks, Dependent Testing and Data Reuse in Large Datasets. Submitted.\n  \n\n  \n    Brad Rodu, Nantaporn Plurphanswat, and Jordan Rodu  (2025)  Inaccurate and misleading meta-analysis of E-cigarettes and population-based diseases.. Internal and Emergency Medicine Volume 20.\n  \n\n  \n    Amelia Bruce Leicht, Xavier Thompson , Robin Queen , Jordan Rodu , Michael Higgins , Kevin Cross , Brian Werner , Jacob Resch , Joseph Hart  (2024)  Comparison of Limb Loading Characteristics and Subjective Functional Outcomes Between Sexes Following ACLR. Journal of Athletic Training Volume 59.\n  \n\n  \n    Jordan Rodu, Alexandra F Dejong Lempke, Natalie Kupperman, Jay Hertel  (2024)  On Leveraging Machine Learning in the Sports Sciences in the Hypothetico-deductive Framework. Sports Medicine – Open Volume 10.\n  \n\n  \n    Bruce Leicht, Amelia S., Xavier D. Thompson, Robin M. Queen, Jordan S. Rodu, Michael J. Higgins, Kevin M. Cross, Brian C. Werner, Jacob E. Resch, and Joe M. Hart  (2024)  Analysis of Limb Loading and Lower Extremity Strength Recovery across Time after Anterior Cruciate Ligament Reconstruction. Sports Health Volume 55.\n  \n\n  \n    Jordan Rodu, Alexandra F Dejong Lempke  Plot panel analysis. Submitted.\n  \n\n  \n    Jordan Rodu, Karen Kafadar  The likelihood ratio plot. Submitted.\n  \n\n  \n    Noah Gade, Jordan Rodu  Nonlinear Permuted Granger Causality. Submitted.\n  \n\n  \n    Noah Gade, Jordan Rodu  Change Point Detection With Conceptors. Submitted.\n  \n\n  \n    Tianyuan Zhou, Joao Sedoc, Jordan Rodu  Markov-Switching Dynamical Systems on Stock Volatility. Submitted.\n  \n\n  \n    Xiaoyuan Ma, Jordan Rodu  Bridging the Usability Gap: Theoretical and Methodological Advances for Spectral Learning of Hidden Markov Models. Submitted.\n  \n\n  \n    Xiaoyuan Ma, Tianyuan Zhou, Jordan Rodu  Locally Hierarchical Graphical Models. Submitted.\n  \n\n  \n    Alexandra F Dejong Lempke, Joseph M Hart, David J Hryvniak, Jordan Rodu, Jay Hertel  (2024)  Transference of Outdoor Gait-Training to Treadmill Running Biomechanics and Strength Measures: A Randomized Controlled Trial.. Journal of Biomechanics Volume 168.\n  \n\n  \n    Jordan Rodu, Michael Baiocchi  (2023)  When black box algorithms are (not) appropriate. Observational Studies.\n  \n\n  \n    Alexandra F Dejong Lempke, Stephanie L Stephens, Pamela N Fish, Xavier D Thompson, Joseph M Hart, David J Hryvniak, Jordan Rodu, Jay Hertel  (2023)  Running-Related Injuries Captured Using Wearable Technology during a Cross-Country Season: A Preliminary Study. Translational Journal of the ACSM 8(1):e000217.\n  \n\n  \n    Alexandra F Dejong Lempke, Stephanie L Stephens, Pamela N Fish, Xavier D Thompson, Joseph M Hart, David J Hryvniak, Jordan Rodu, Jay Hertel  (2022)  Sensor-based gait training to reduce contact time for runners with exercise-related lower leg pain: a randomised controlled trial. BMJ Open Sport and Exercise Medicine 3;8(4):e001293.\n  \n\n  \n    Sean A Klein, Michael Baiocchi, Jordan Rodu, Heather Baker, Erica Rosemond, Jamie Mihoko Doyle  (2022)  An analysis of the Clinical and Translational Science Award pilot project portfolio using data from Research Performance Progress Reports. Journal of Clinical and Translational Science 18;6(1):e113.\n  \n\n  \n    Alexandra F Dejong Lempke, Joseph M Hart, David J Hryvniak, Jordan Rodu, Jay Hertel  (2022)  Prospective running assessments among division I cross-country athletes. Physical Therapy in Sport Volume 55.\n  \n\n  \n    Alexandra F Dejong Lempke, Joseph M Hart, David J Hryvniak, Jordan Rodu, Jay Hertel  (2021)  Use of wearable sensors to identify biomechanical alterations in runners with Exercise-Related lower leg pain. Journal of Biomechanics Volume 126.\n  \n\n  \n    Jordan Rodu, Karen Kafadar  (2021)  The q–q Boxplot. Journal of Computational and Graphical Statistics 31:1, 26-39.\n  \n\n  \n    Jordan Rodu  (2021)  Text Analytics: Advances and Challenges Domenica Fioredistella Iezzi, Damon Mayaffre and Michelangelo Misuraca Springer, 2020, xi + 302 pages, £95.50, eBook ISBN: 978-3-030-52680-1. International Statistical Review Volume 89, Issue 2.\n  \n\n  \n    Michael Baiocchi, Jordan Rodu  (2021)  Reasoning Using Data: Two Old Ways and One New. Observational Studies Volume 7, Issue 1.\n  \n\n  \n    Tianyuan Zhou, Joao Sedoc, Jordan Rodu  (2019)  Getting in Shape: Word Embedding SubSpaces. Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence.\n  \n\n  \n    Jordan Rodu, Natalie Klein, Scott L. Brincat, Earl K. Miller, Robert E. Kass  (2018)  Detecting multivariate cross-correlation between brain regions. Journal of Neurophysiology 120(4):1962-1972.\n  \n\n  \n    Robert E. Kass et. al  (2018)  Computational Neuroscience: Mathematical and Statistical Perspectives. Annual Review of Statistics and Its Application Vol. 5:183-214.\n  \n\n  \n    Josue Orellana, Jordan Rodu, Robert E. Kass  (2017)  Population Vectors Can Provide Near Optimal Integration of Information. Neural Computation 29(8):2021-2029.\n  \n\n  \n    Jordan Rodu, Dean P. Foster, Weichen Wu, Lyle H. Ungar  (2013)  Using Regression for Spectral Estimation of HMMs. Statistical Language and Speech Processing - First International Conference. Proceedings. Lecture Notes in Computer Science 7978, Springer.\n  \n\n  \n    Paramveer S. Dhillon, Jordan Rodu, Michael Collins, Dean P. Foster, Lyle H. Ungar  (2012)  Spectral Dependency Parsing with Latent Variables. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.\n  \n\n  \n    Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster, Lyle H. Ungar  (2012)  Two step CCA: a new spectral method for estimating vector models of words. Proceedings of the 29th International Coference on International Conference on Machine Learning.\n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jordan Rodu",
    "section": "",
    "text": "Jordan Rodu Department of Statistics The University of Virginia Charlottesville, VA\nI am an associate professor in the Department of Statistics at the University of Virginia. My research interests include statistical visualization, theory of reasoning with data, and time series. I spend a lot of time thinking about the fuzzy space between statistics and machine learning/AI."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About me",
    "section": "",
    "text": "Jordan Rodu Department of Statistics The University of Virginia Charlottesville, VA\n\n\n\nPhD in Statistics, 2014\nThe Wharton School at the University of Pennsylvania, Philadelphia, PA\nMA in Orchestral Conducting, 2006\nBard College, Annandale-on-Hudson, NY\nBA in Mathematics, 2005\nWilliams College, Williamstown, MA\n\nmore soon…"
  },
  {
    "objectID": "about/index.html#education",
    "href": "about/index.html#education",
    "title": "About me",
    "section": "",
    "text": "PhD in Statistics, 2014\nThe Wharton School at the University of Pennsylvania, Philadelphia, PA\nMA in Orchestral Conducting, 2006\nBard College, Annandale-on-Hudson, NY\nBA in Mathematics, 2005\nWilliams College, Williamstown, MA\n\nmore soon…"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "My blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index_bak.html",
    "href": "index_bak.html",
    "title": "Jordan Rodu",
    "section": "",
    "text": "Jordan Rodu is an assistant professor of Statistics at the University of Virginia. His primary interests include statistical visualization, methods of reasoning with data, and time series."
  },
  {
    "objectID": "index_bak.html#education",
    "href": "index_bak.html#education",
    "title": "Jordan Rodu",
    "section": "Education",
    "text": "Education\nPhD, Statistics, 2014\nThe Wharton School at the University of Pennsylvania\nPhiladelphia, PA\nBA, Mathematics, 2005\nWilliams College\nWilliamstown, MA"
  },
  {
    "objectID": "index_bak.html#experience",
    "href": "index_bak.html#experience",
    "title": "Jordan Rodu",
    "section": "Experience",
    "text": "Experience\nAssistant Professor\nDepartment of Statistics\nUniversity of Virginia, Charlottesville VA\n2017-present\nVisiting Assistant Professor\nDepartment of Statistics\nCarnegie Mellon University, Pittsburgh, PA\n2014-2017"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Selected R packages",
    "section": "",
    "text": "Selected R packages\n\n\n\n\nPPA: a package that contains an example  shiny app for plot panel analysis\n\ngithub\n\n\n\nqqboxplot: an implementation of  the q-q boxplot\n\ngithub, CRAN"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Talks\nComing soon…"
  },
  {
    "objectID": "publications/MARA/index.html",
    "href": "publications/MARA/index.html",
    "title": "When black box algorithms are (not) appropriate",
    "section": "",
    "text": "In the 1980s a new, extraordinarily productive way of reasoning about algorithms emerged. In this paper, we introduce the term “outcome reasoning” to refer to this form of reasoning. Though outcome reasoning has come to dominate areas of data science, it has been under-discussed and its impact under-appreciated. For example, outcome reasoning is the primary way we reason about whether “black box” algorithms are performing well. In this paper we analyze outcome reasoning’s most common form (i.e., as “the common task framework”) and its limitations. We discuss why a large class of prediction-problems are inappropriate for outcome reasoning. As an example, we find the common task framework does not provide a foundation for the deployment of an algorithm in a real world situation. Building off of its core features, we identify a class of problems where this new form of reasoning can be used in deployment. We purposefully develop a novel framework so both technical and non-technical people can discuss and identify key features of their prediction problem and whether or not it is suitable for outcome reasoning."
  },
  {
    "objectID": "publications/computational-neuroscience-annual-reviews/index.html",
    "href": "publications/computational-neuroscience-annual-reviews/index.html",
    "title": "Computational Neuroscience: Mathematical and Statistical Perspectives",
    "section": "",
    "text": "Mathematical and statistical models have played important roles in neuroscience, especially by describing the electrical activity of neurons recorded individually, or collectively across large networks. As the field moves forward rapidly, new challenges are emerging. For maximal effectiveness, those working to advance computational neuroscience will need to appreciate and exploit the complementary strengths of mechanistic theory and the statistical paradigm."
  },
  {
    "objectID": "publications/population-vectors/index.html",
    "href": "publications/population-vectors/index.html",
    "title": "Population Vectors Can Provide Near Optimal Integration of Information",
    "section": "",
    "text": "Much attention has been paid to the question of how Bayesian integration of information could be implemented by a simple neural mechanism. We show that population vectors based on point-process inputs combine evidence in a form that closely resembles Bayesian inference, with each input spike carrying information about the tuning of the input neuron. We also show that population vectors can combine information relatively accurately in the presence of noisy synaptic encoding of tuning curves."
  },
  {
    "objectID": "publications/qqboxplot/index.html",
    "href": "publications/qqboxplot/index.html",
    "title": "The q–q Boxplot",
    "section": "",
    "text": "Boxplots have become an extremely popular display of distribution summaries for collections of data, especially when we need to visualize summaries for several collections simultaneously. The whiskers in the boxplot show only the extent of the tails for most of the data (with outside values denoted separately); more detailed information about the shape of the tails, such as skewness and “weight” relative to a standard reference distribution, is much better displayed via quantile–quantile (q-q) plots. We incorporate the q-q plot’s tail information into the traditional boxplot by replacing the boxplot’s whiskers with the tails from a q-q plot, and display these tails with confidence bands for the tails that would be expected from the tails of the reference distribution. We describe the construction of the “q-q boxplot” and demonstrate its advantages over earlier proposed boxplot modifications on data from economics and neuroscience, which illustrate the q-q boxplots’ effectiveness in showing important tail behavior especially for large datasets. The package qqboxplot (an extension to the ggplot2 package) is available for the R programming language. Supplementary files for this article are available online."
  },
  {
    "objectID": "publications/spectral-dependency-parsing/index.html",
    "href": "publications/spectral-dependency-parsing/index.html",
    "title": "Spectral Dependency Parsing with Latent Variables",
    "section": "",
    "text": "Recently there has been substantial interest in using spectral methods to learn generative sequence models like HMMs. Spectral methods are attractive as they provide globally consistent estimates of the model parameters and are very fast and scalable, unlike EM methods, which can get stuck in local minima. In this paper, we present a novel extension of this class of spectral methods to learn dependency tree structures. We propose a simple yet powerful latent variable generative model for dependency parsing, and a spectral learning method to efficiently estimate it. As a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-theart parser. Our approach gives us a moderate reduction in error of up to 4.6% over the baseline re-ranker."
  },
  {
    "objectID": "publications/translational-science-award/index.html",
    "href": "publications/translational-science-award/index.html",
    "title": "An analysis of the Clinical and Translational Science Award pilot project portfolio using data from Research Performance Progress Reports",
    "section": "",
    "text": "Pilot projects (“pilots”) are important for testing hypotheses in advance of investing more funds for full research studies. For some programs, such as Clinical and Translational Science Awards (CTSAs) supported by the National Center for Translational Sciences, pilots also make up a significant proportion of the research projects conducted with direct CTSA support. Unfortunately, administrative data on pilots are not typically captured in accessible databases. Though data on pilots are included in Research Performance Progress Reports, it is often difficult to extract, especially for large programs like the CTSAs where more than 600 pilots may be reported across all awardees annually. Data extraction challenges preclude analyses that could provide valuable information about pilots to researchers and administrators."
  },
  {
    "objectID": "teaching/index.html#capstone",
    "href": "teaching/index.html#capstone",
    "title": "Jordan Rodu",
    "section": "",
    "text": "UVA STAT 4996\nFall 23 Spring 20, 23, 24"
  },
  {
    "objectID": "teaching/index.html#applied-causal-inference",
    "href": "teaching/index.html#applied-causal-inference",
    "title": "Courses at UVA",
    "section": "",
    "text": "This is a course that had not been taught since prior to my joining UVA,\nso I designed a new course.\nUVA STAT 5350\nFall 22, 23"
  },
  {
    "objectID": "teaching/index.html#advanced-sports-analytics",
    "href": "teaching/index.html#advanced-sports-analytics",
    "title": "Courses at UVA",
    "section": "",
    "text": "This is a course I created at UVA.\nUVA STAT 4800\nFall 21, 22\nSpring 20, 21"
  },
  {
    "objectID": "teaching/index.html#introduction-to-sports-analytics",
    "href": "teaching/index.html#introduction-to-sports-analytics",
    "title": "Courses at UVA",
    "section": "",
    "text": "This is a course I created at UVA.\nUVA STAT 1800\nFall 19, 21"
  },
  {
    "objectID": "teaching/index.html#statistical-text-analysis",
    "href": "teaching/index.html#statistical-text-analysis",
    "title": "Courses at UVA",
    "section": "",
    "text": "This is a course I created at UVA.\nUVA STAT 4559\nFall 20"
  },
  {
    "objectID": "teaching/index.html#data-visualization-and-presentation",
    "href": "teaching/index.html#data-visualization-and-presentation",
    "title": "Courses at UVA",
    "section": "",
    "text": "This is a course I created at UVA. Data visualization is\nnow being taught in along with databases (STAT 3280)\nUVA STAT 4310\nFall 17, 18\nSpring 19"
  },
  {
    "objectID": "teaching/index.html#statistical-machine-learning-graduate-level",
    "href": "teaching/index.html#statistical-machine-learning-graduate-level",
    "title": "Courses at UVA",
    "section": "",
    "text": "UVA STAT 5630\nFall 18"
  },
  {
    "objectID": "teaching/index.html#statistical-machine-learning-undergraduate",
    "href": "teaching/index.html#statistical-machine-learning-undergraduate",
    "title": "Courses at UVA",
    "section": "",
    "text": "UVA STAT 4630\nSpring 18"
  },
  {
    "objectID": "teaching/index.html#section",
    "href": "teaching/index.html#section",
    "title": "Courses at UVA",
    "section": "",
    "text": "This is a course I created at UVA.\nUVA STAT 4800\nFall 21, 22\nSpring 20, 21"
  },
  {
    "objectID": "publications/nonlinear-permuted-granger-causality/index.html",
    "href": "publications/nonlinear-permuted-granger-causality/index.html",
    "title": "Nonlinear Permuted Granger Causality",
    "section": "",
    "text": "Granger causal inference is a contentious but widespread method used in fields ranging from economics to neuroscience. The original definition addresses the notion of causality in time series by establishing functional dependence conditional on a specified model. Adaptation of Granger causality to nonlinear data remains challenging, and many methods apply in-sample tests that do not incorporate out-of-sample predictability, leading to concerns of model overfitting. To allow for out-of-sample comparison, a measure of functional connectivity is explicitly defined using permutations of the covariate set. Artificial neural networks serve as featurizers of the data to approximate any arbitrary, nonlinear relationship, and consistent estimation of the variance for each permutation is shown under certain conditions on the featurization process and the model residuals. Performance of the permutation method is compared to penalized variable selection, naive replacement, and omission techniques via simulation, and it is applied to neuronal responses of acoustic stimuli in the auditory cortex of anesthetized rats. Targeted use of the Granger causal framework, when prior knowledge of the causal mechanisms in a dataset are limited, can help to reveal potential predictive relationships between sets of variables that warrant further study."
  },
  {
    "objectID": "publications/change-point-conceptors/index.html",
    "href": "publications/change-point-conceptors/index.html",
    "title": "Change Point Detection With Conceptors",
    "section": "",
    "text": "Offline change point detection seeks to identify points in a time series where the data generating process changes. This problem is well studied for univariate i.i.d. data, but becomes challenging with increasing dimension and temporal dependence. For the at most one change point problem, we propose the use of a conceptor matrix to learn the characteristic dynamics of a specified training window in a time series. The associated random recurrent neural network acts as a featurizer of the data, and change points are identified from a univariate quantification of the distance between the featurization and the space spanned by a representative conceptor matrix. This model agnostic method can suggest potential locations of interest that warrant further study. We prove that, under mild assumptions, the method provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on simulations from several classes of processes, and we evaluate performance with clustering metrics, graphical methods, and observed Type 1 error control. We apply our method to publicly available neural data from rats experiencing bouts of non-REM sleep prior to exploration of a radial maze."
  },
  {
    "objectID": "publications/markov-swithcing-volatility/index.html",
    "href": "publications/markov-swithcing-volatility/index.html",
    "title": "Markov-Switching Dynamical Systems on Stock Volatility",
    "section": "",
    "text": "The Markov-switching phenomenon for stock volatility is widely studied, however, little attention has been paid to switching between different model classes, such as a Heston model and a 3/2 model. In this article, we propose a novel heteroskedasticity-based E-M algorithm for inferring such a model-class switching model, and a testing procedure to demonstrate the existence of such a switching phenomenon. We verify our proposed method through both simulation and real data analysis of both the implied and realized volatility of the stock market."
  },
  {
    "objectID": "publications/projected-spectral-hmm/index.html",
    "href": "publications/projected-spectral-hmm/index.html",
    "title": "Bridging the Usability Gap: Theoretical and Methodological Advances for Spectral Learning of Hidden Markov Models",
    "section": "",
    "text": "The Baum-Welch (B-W) algorithm is the most widely accepted method for inferring hidden Markov models (HMM). However, it is prone to getting stuck in local optima, and can be too slow for many real-time applications. Spectral learning of HMMs (SHMM), based on the method of moments (MOM) has been proposed in the literature to overcome these obstacles. Despite its promises, asymptotic theory for SHMM has been elusive, and the long-run performance of SHMM can degrade due to unchecked propagation of error. In this paper, we (1) provide an asymptotic distribution for the approximate error of the likelihood estimated by SHMM, (2) propose a novel algorithm called projected SHMM (PSHMM) that mitigates the problem of error propagation, and (3) describe online learning variants of both SHMM and PSHMM that accommodate potential nonstationarity. We compare the performance of SHMM with PSHMM and estimation through the B-W algorithm on both simulated data and data from real world applications, and find that PSHMM not only retains the computational advantages of SHMM, but also provides more robust estimation and forecasting."
  },
  {
    "objectID": "publications/lr-plot/index.html",
    "href": "publications/lr-plot/index.html",
    "title": "The likelihood ratio plot",
    "section": "",
    "text": "The relationships between posterior odds, likelihood ratios and prior odds can be difficult to communicate. Further, the effect of the uncertainties in these quantities, especially on the posterior odds, is greatly underappreciated. This article proposes a plot that displays the relationship between the likelihood ratio and prior odds, and the resulting posterior odds, that incorporates uncertainties. The plot is easy to construct and is very useful in communicating, to both scientists and nonscientists, the weight of evidence in multiple settings, especially in judicial settings where ``transposing the conditionals’’ (misinterpreting the likelihood ratio as the posterior odds) frequently occurs. Illustrations of the plot, constructed using a cited, freely available R package, demonstrate its advantages."
  },
  {
    "objectID": "publications/ppa/index.html",
    "href": "publications/ppa/index.html",
    "title": "Plot panel analysis",
    "section": "",
    "text": "Modern data analysis frequently requires characterizing a large collection of observations composed of multiple data points (e.g. collections of time series, scatterplots, or networks). Data visualization is an essential tool for exploring such data, but novel techniques are needed to overcome challenges associated with visualizing populations of these observations, such as overplotting. This manuscript proposes a novel approach that we call plot panel analysis (PPA). PPA is an interactive small multiples technique that promotes clarity in simultaneous visual access to the entire population of observations, efficient bookkeeping, and flexible feature examination and characterization. We call the mechanism that facilitates these objectives ``interactive selection.’’ Motivated by the rise in popularity of and challenges working with data from wearable sensors in Kinesiology studies, we demonstrate the effectiveness of PPA with a simple, easy-to-understand simulated example based on real data from Runscribe sensors. A usable prototype R Shiny application, ppa, is provided."
  },
  {
    "objectID": "publications/ml-in-ss/index.html",
    "href": "publications/ml-in-ss/index.html",
    "title": "On Leveraging Machine Learning in the Sports Sciences in the Hypothetico-deductive Framework",
    "section": "",
    "text": "Supervised machine learning (ML) offers an exciting suite of algorithms that could benefit research in the sports sciences. In principle, supervised ML approaches were designed for pure prediction, as opposed to explanation, leading to a rise in powerful, but opaque, algorithms. Recently, two subdomains of ML–explainable ML, which allows us to “peek into the black box,” and interpretable ML, which encourages using algorithms that are inherently interpretable–have grown in popularity. Given this increased transparency into ML algorithms, can supervised ML be used in place of statistical methods in the hypothetico-deductive framework? This paper shows why ML algorithms are fundamentally different from statistical methods, even when using explainable or interpretable approaches. While supervised ML cannot be used in place of statistical methods, we propose ways in which the sports science community can take advantage of supervised ML in the hypothetico-deductive framework. In this manuscript we argue that supervised machine learning can and should augment our exploratory investigations in the sports sciences, but should not replace statistical reasoning in the hypothetico-deductive framework. We justify our position through a careful examination of supervised machine learning, and provide a useful analogy to help elucidate our findings. Three case studies are provided to demonstrate how supervised machine learning can be integrated into exploratory analysis. Supervised machine learning should be integrated into the scientific workflow with requisite caution. The approaches described in this paper provide ways to safely leverage the strengths of machine learning while avoiding potential pitfalls."
  },
  {
    "objectID": "publications/lhgm/index.html",
    "href": "publications/lhgm/index.html",
    "title": "Locally Hierarchical Graphical Models",
    "section": "",
    "text": "Markov networks, constructed from RNA sequencing data, encode the dependency structure between genes, which is essential for gene association detection and targeted drug development. The graphical lasso and its variants, including the Poisson graphical lasso, represent the state-of-the-art for building Markov networks. However, these methods assume a certain homogeneity in the number of associations for each gene across the network, expressed through a globally optimal sparsity penalty. We propose a method we called Locally Hierarchical Graphical Models (LHGM) which puts a prior on the sparsity instead of controlling sparsity through a lasso. We estimate the LHGM through an algorithm we call Adaptive Search by Lasso. We highlight the superior performance of LHGM on heterogeneous data through both simulation and application on TCGA-UCEC data."
  },
  {
    "objectID": "publications/data-gluttony/index.html",
    "href": "publications/data-gluttony/index.html",
    "title": "Data Gluttony: Epistemic Risks, Dependent Testing and Data Reuse in Large Datasets",
    "section": "",
    "text": "Large-scale registries have collected vast amounts of data which has enabled investigators to efficiently conduct studies of observational data. However, we demonstrate how data reuse leads to positive dependence among the inferential tasks and cascading inferential errors.\nCommon practice is for investigators to use all data meeting the inclusion criteria of their study to perform their analysis. We term this common practice data gluttony. It has apparent formal justification insofar as this approach maximizes per-study power. But this comes at a cost: data reuse affects the shape of the distribution of inferential errors. Using the theory of risk orderings we demonstrate how positively dependent testing procedures result in strictly riskier distributions of inferential error.\nWe identify two remedies to this state of affairs: research portfolio optimization and what we term data temperance. Research portfolio optimization requires that we formulate the enterprise of inference in a utility theoretic framework: associated to each hypothesis to be evaluated is some utility dependent on its truth as well as the impact of the statistical decision rendered on the basis of the data. Under certain models of data governance, this approach can be used to optimally allocate data usage across multiple inferential tasks.\nOn the other hand, data temperance is a more flexible strategy for managing the distribution of inferential errors. Data temperance is the principle that an investigator use only as much data as is necessary to perform the task at hand. This is possible due to the diminishing marginal returns in power and precision in sample size. In contrast to portfolio optimization, data temperance can be effectively applied in a federated manner. Moreover, we analyze the effectiveness of data temperance at reducing the dependence across testing and develop a theory of the capacity of a static database to sustain large numbers of inferential tasks with low probability of inducing pairwise dependent testing procedures.\nRegistries are an invaluable resource. We must ensure that they remain valuable by mitigating the risk of cascading inferential errors."
  }
]